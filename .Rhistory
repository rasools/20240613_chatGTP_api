# Function to query OpenAI API
chatGPT <- function(prompt, apiKey, modelName = "gpt-3.5-turbo", temperature = 1) {
response <- POST(
url = "https://api.openai.com/v1/completions",
add_headers(Authorization = paste("Bearer", apiKey)),
content_type_json(),
encode = "json",
body = list(
model = modelName,
temperature = temperature,
max_tokens = 150,
prompt = prompt
)
)
stop_for_status(response)
content(response)$choices[[1]]$text
}
# Read the list of genes from the Excel file
path <- "genes.xlsx"
genes_df <- read_excel(path)
genes_list <- genes_df$genes  # Assuming 'genes' is the header for the first column
# Initialize a list to store the results
results <- vector("list", length(genes_list))
names(results) <- genes_list
# Loop through each gene and get knockout cell information
for (gene in genes_list) {
prompt <- paste("List reported knockout cells for the gene", gene, "and the titles of the publications.")
response <- chatGPT(prompt, Sys.getenv("OPENAI_API_KEY_NBIS"))
results[[gene]] <- response
Sys.sleep(1)  # To prevent rate limiting
}
library(httr)
library(jsonlite)
library(openxlsx)
library(readxl)
# Function to query OpenAI API
chatGPT <- function(prompt, apiKey, modelName = "gpt-3.5-turbo", temperature = 1) {
response <- POST(
url = "https://api.openai.com/v1/completions",
add_headers(Authorization = paste("Bearer", apiKey)),
content_type_json(),
encode = "json",
body = list(
model = modelName,
temperature = temperature,
max_tokens = 150,
prompt = prompt
)
)
stop_for_status(response)
content(response)$choices[[1]]$text
}
# Read the list of genes from the Excel file
path <- "genes.xlsx"
genes_df <- read_excel(path)
genes_list <- genes_df$genes  # Assuming 'genes' is the header for the first column
# Initialize a list to store the results
results <- vector("list", length(genes_list))
names(results) <- genes_list
# Loop through each gene and get knockout cell information
for (gene in genes_list) {
prompt <- paste("List reported knockout cells for the gene", gene, "and the titles of the publications.")
response <- chatGPT(prompt, Sys.getenv("OPENAI_API_KEY_NBIS"))
results[[gene]] <- response
Sys.sleep(1)  # To prevent rate limiting
}
Sys.getenv("OPENAI_API_KEY_NBIS"
)
# Function to query OpenAI API
chatGPT <- function(prompt, apiKey, modelName = "gpt-3.5-turbo", temperature = 1) {
response <- POST(
url = "https://api.openai.com/v1/completions",
add_headers(Authorization = paste("Bearer", apiKey)),
content_type_json(),
encode = "json",
body = list(
model = modelName,
temperature = temperature,
max_tokens = 150,
prompt = prompt
)
)
stop_for_status(response)
content(response)$choices[[1]]$text
}
# Read the list of genes from the Excel file
path <- "Ex2/genes.xlsx"
genes_df <- read_excel(path)
genes_list <- genes_df$genes  # Assuming 'genes' is the header for the first column
# Initialize a list to store the results
results <- vector("list", length(genes_list))
names(results) <- genes_list
# Loop through each gene and get knockout cell information
for (gene in genes_list) {
prompt <- paste("List reported knockout cells for the gene", gene, "and the titles of the publications.")
response <- chatGPT(prompt, Sys.getenv("OPENAI_API_KEY_NBIS"))
results[[gene]] <- response
Sys.sleep(1)  # To prevent rate limiting
}
Sys.getenv("OPENAI_API_KEY_NBIS")
source("Ex2/svenski_shiny_ui.R")
source("Ex2/svenski_shiny.R")
pwd
getwd()
library(httr)
library(jsonlite)
# Function to list the folder structure and optionally read file contents
get_folder_structure <- function(dir_path, read_files = FALSE) {
folder_structure <- list()
files_and_dirs <- list.files(dir_path, full.names = TRUE)
for (entry in files_and_dirs) {
entry_name <- basename(entry)
if (file.info(entry)$isdir) {
folder_structure[[entry_name]] <- get_folder_structure(entry, read_files)
} else {
if (read_files) {
file_content <- tryCatch({
readLines(entry)
}, error = function(e) {
paste("Error reading file:", e)
})
folder_structure[[entry_name]] <- paste(file_content, collapse="\n")
} else {
folder_structure[[entry_name]] <- NULL
}
}
}
return(folder_structure)
}
# Folder structure
dir_path <- "/Users/rasools/sdrive/projects/LTS_Frisen/Frisen_gdrive/nbis_frisen_2305"
folder_structure <- get_folder_structure(dir_path, read_files = FALSE)
# Function to interact with ChatGPT and retrieve the response
chat_gpt_interaction <- function(message, api_key, folder_structure = NULL) {
url <- "https://api.openai.com/v1/chat/completions"
context_message <- if (!is.null(folder_structure)) {
paste("Here is the folder structure and files:\n", toJSON(folder_structure, pretty = TRUE), "\n\nLet's discuss:")
} else {
""
}
body <- list(
model = "gpt-3.5-turbo",
messages = list(
list(role = "system", content = context_message),  # Context message here
list(role = "user", content = message)
),
max_tokens = 150,
n = 1,
temperature = 0.7
)
response <- POST(
url,
add_headers("Authorization" = paste("Bearer", api_key), "Content-Type" = "application/json"),
body = toJSON(body, auto_unbox = TRUE),
encode = "json"
)
if (status_code(response) != 200) {
stop("API request failed with status code ", status_code(response), ": ", content(response, as = "text", encoding = "UTF-8"))
}
content <- content(response, as = "text", encoding = "UTF-8")
json_content <- fromJSON(content, flatten = TRUE)
if (length(json_content$choices) > 0 && "message.content" %in% colnames(json_content$choices)) {
response_text <- json_content$choices[1, "message.content"]
total_tokens <- json_content$usage$total_tokens[1]
} else {
stop("Unexpected response format from API")
}
return(list(response_text = response_text, total_tokens = total_tokens))
}
initial_message <- "Let's discuss the types of data available in the project folder."
message <- initial_message
total_tokens <- 0
continue_chat <- TRUE
# Chat loop
while (continue_chat) {
cat("You say:\n", message, "\n\n")
response <- chat_gpt_interaction(message, Sys.getenv("OPENAI_API_KEY_NBIS"), folder_structure)
chat_gpt_response <- response$response_text
total_tokens <- total_tokens + response$total_tokens
cat("ChatGPT says:\n", chat_gpt_response, "\n\n")
Sys.sleep(1)
message <- readline("Your response (type 'exit' to end chat): ")
if (tolower(message) == "exit") {
cat("You have chosen to end the chat.\n")
continue_chat <- FALSE
} else if (message == "") {
cat("No input provided. Ending chat.\n")
continue_chat <- FALSE
}
}
cat("Total tokens used: ", total_tokens, "\n")
cat("Chat session ended.\n")
library(httr)
library(jsonlite)
library(readxl)
library(knitr)
library(kableExtra)
chatGPT <- function(prompt, apiKey, modelName = "gpt-3.5-turbo", temperature = 1) {
response <- POST(
url = "https://api.openai.com/v1/chat/completions",
add_headers(Authorization = paste("Bearer", apiKey)),
content_type_json(),
encode = "json",
body = list(
model = modelName,
temperature = temperature,
messages = list(list(role = "user", content = prompt))
)
)
stop_for_status(response)
trimws(content(response)$choices[[1]]$message$content)
}
path <- "genes.xlsx"
genes <- read_excel(path, col_names = TRUE)
genes_list <- as.character(genes[[1]])
results <- list()
for (gene in genes_list) {
prompt <- paste("What are the reported knockout cells for the gene", gene, "and the titles of the publications?")
response <- chatGPT(prompt, Sys.getenv("OPENAI_API_KEY_NBIS"))
results[[gene]] <- response
}
df <- data.frame(Gene = character(), Response = character(), stringsAsFactors = FALSE)
for (gene in genes_list) {
df <- rbind(df, data.frame(Gene = gene, Response = results[[gene]], stringsAsFactors = FALSE))
}
kable(df, escape = FALSE) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
library(shiny); runApp('Ex2/svenski_shiny.R')
library(httr)
library(jsonlite)
chat_gpt_interaction <- function(messages, api_key, model, temperature = 0.7) {
url <- "https://api.openai.com/v1/chat/completions"
body <- list(
model = model,
messages = messages,
max_tokens = 150,
temperature = temperature  # Set the temperature
)
response <- POST(
url,
add_headers(
`Authorization` = paste("Bearer", api_key),
`Content-Type` = "application/json"
),
body = toJSON(body, auto_unbox = TRUE),
encode = "json"
)
if (status_code(response) != 200) {
stop("API request failed with status code ", status_code(response),
": ", content(response, as = "text", encoding = "UTF-8"))
}
# Extract content as text and parse JSON
content_as_text <- content(response, as = "text", encoding = "UTF-8")
content <- fromJSON(content_as_text)
# Ensure that 'choices' has at least one item
if (length(content[["choices"]]) < 1) {
stop("No choices found in the response.")
}
# Extract the message content using the correct path
response_text <- content[["choices"]][["message"]][["content"]]
if (is.null(response_text)) {
stop("Unable to extract message content from the choice")
}
total_tokens <- content[["usage"]][["total_tokens"]]
return(list(response_text = response_text, total_tokens = total_tokens))
}
library(httr)
library(jsonlite)
chat_gpt_interaction <- function(messages, api_key, model, temperature = 0.7) {
url <- "https://api.openai.com/v1/chat/completions"
body <- list(
model = model,
messages = messages,
max_tokens = 150,
temperature = temperature  # Set the temperature
)
response <- POST(
url,
add_headers(
`Authorization` = paste("Bearer", api_key),
`Content-Type` = "application/json"
),
body = toJSON(body, auto_unbox = TRUE),
encode = "json"
)
if (status_code(response) != 200) {
stop("API request failed with status code ", status_code(response),
": ", content(response, as = "text", encoding = "UTF-8"))
}
# Extract content as text and parse JSON
content_as_text <- content(response, as = "text", encoding = "UTF-8")
content <- fromJSON(content_as_text)
# Ensure that 'choices' has at least one item
if (length(content[["choices"]]) < 1) {
stop("No choices found in the response.")
}
# Extract the message content using the correct path
response_text <- content[["choices"]][["message"]][["content"]]
if (is.null(response_text)) {
stop("Unable to extract message content from the choice")
}
total_tokens <- content[["usage"]][["total_tokens"]]
return(list(response_text = response_text, total_tokens = total_tokens))
}
append_message <- function(context, role, content) {
if (!is.character(content)) {
stop("Content must be a string.")
}
new_message <- list(role = role, content = content)
return(append(context, list(new_message)))
}
append_message <- function(context, role, content) {
if (!is.character(content)) {
stop("Content must be a string.")
}
new_message <- list(role = role, content = content)
return(append(context, list(new_message)))
}
library(httr)
library(jsonlite)
chat_gpt_interaction <- function(messages, api_key, model, temperature = 0.7) {
url <- "https://api.openai.com/v1/chat/completions"
body <- list(
model = model,
messages = messages,
max_tokens = 150,
temperature = temperature  # Set the temperature
)
response <- POST(
url,
add_headers(
`Authorization` = paste("Bearer", api_key),
`Content-Type` = "application/json"
),
body = toJSON(body, auto_unbox = TRUE),
encode = "json"
)
if (status_code(response) != 200) {
stop("API request failed with status code ", status_code(response),
": ", content(response, as = "text", encoding = "UTF-8"))
}
# Extract content as text and parse JSON
content_as_text <- content(response, as = "text", encoding = "UTF-8")
content <- fromJSON(content_as_text)
# Ensure that 'choices' has at least one item
if (length(content[["choices"]]) < 1) {
stop("No choices found in the response.")
}
# Extract the message content using the correct path
response_text <- content[["choices"]][["message"]][["content"]]
if (is.null(response_text)) {
stop("Unable to extract message content from the choice")
}
total_tokens <- content[["usage"]][["total_tokens"]]
return(list(response_text = response_text, total_tokens = total_tokens))
}
append_message <- function(context, role, content) {
if (!is.character(content)) {
stop("Content must be a string.")
}
new_message <- list(role = role, content = content)
return(append(context, list(new_message)))
}
get_folder_structure <- function(dir_path, read_files = FALSE) {
folder_structure <- list()
files_and_dirs <- list.files(dir_path, full.names = TRUE)
for (entry in files_and_dirs) {
entry_name <- basename(entry)
if (file.info(entry)$isdir) {
folder_structure[[entry_name]] <- get_folder_structure(entry, read_files)
} else {
if (read_files) {
file_content <- tryCatch({
readLines(entry)
}, error = function(e) {
paste("Error reading file:", e$message)
})
folder_structure[[entry_name]] <- paste(file_content, collapse = "\n")
} else {
folder_structure[[entry_name]] <- NULL
}
}
}
return(folder_structure)
}
folder_structure_to_string <- function(folder_structure) {
folder_string <- ""
for (entry_name in names(folder_structure)) {
item <- folder_structure[[entry_name]]
if (is.null(item)) {
folder_string <- paste(folder_string, entry_name, "(File)", sep = "\n")
} else {
folder_string <- paste(folder_string, entry_name, "(Folder):", sep = "\n")
folder_string <- paste(folder_string, folder_structure_to_string(item), sep = "\n")
}
}
return(folder_string)
}
library(httr)
library(jsonlite)
chat_gpt_interaction <- function(messages, api_key, model, temperature = 0.7) {
url <- "https://api.openai.com/v1/chat/completions"
body <- list(
model = model,
messages = messages,
max_tokens = 150,
temperature = temperature  # Set the temperature
)
response <- POST(
url,
add_headers(
`Authorization` = paste("Bearer", api_key),
`Content-Type` = "application/json"
),
body = toJSON(body, auto_unbox = TRUE),
encode = "json"
)
if (status_code(response) != 200) {
stop("API request failed with status code ", status_code(response),
": ", content(response, as = "text", encoding = "UTF-8"))
}
# Extract content as text and parse JSON
content_as_text <- content(response, as = "text", encoding = "UTF-8")
content <- fromJSON(content_as_text)
# Ensure that 'choices' has at least one item
if (length(content[["choices"]]) < 1) {
stop("No choices found in the response.")
}
# Extract the message content using the correct path
response_text <- content[["choices"]][["message"]][["content"]]
if (is.null(response_text)) {
stop("Unable to extract message content from the choice")
}
total_tokens <- content[["usage"]][["total_tokens"]]
return(list(response_text = response_text, total_tokens = total_tokens))
}
append_message <- function(context, role, content) {
if (!is.character(content)) {
stop("Content must be a string.")
}
new_message <- list(role = role, content = content)
return(append(context, list(new_message)))
}
get_folder_structure <- function(dir_path, read_files = FALSE) {
folder_structure <- list()
files_and_dirs <- list.files(dir_path, full.names = TRUE)
for (entry in files_and_dirs) {
entry_name <- basename(entry)
if (file.info(entry)$isdir) {
folder_structure[[entry_name]] <- get_folder_structure(entry, read_files)
} else {
if (read_files) {
file_content <- tryCatch({
readLines(entry)
}, error = function(e) {
paste("Error reading file:", e$message)
})
folder_structure[[entry_name]] <- paste(file_content, collapse = "\n")
} else {
folder_structure[[entry_name]] <- NULL
}
}
}
return(folder_structure)
}
folder_structure_to_string <- function(folder_structure) {
folder_string <- ""
for (entry_name in names(folder_structure)) {
item <- folder_structure[[entry_name]]
if (is.null(item)) {
folder_string <- paste(folder_string, entry_name, "(File)", sep = "\n")
} else {
folder_string <- paste(folder_string, entry_name, "(Folder):", sep = "\n")
folder_string <- paste(folder_string, folder_structure_to_string(item), sep = "\n")
}
}
return(folder_string)
}
# Configuration and initial setup
model <- "gpt-3.5-turbo"
# model <- "gpt-4o"
# Get the folder structure in textual form
dir_path <- "/Users/rasools/sdrive/projects/LTS_Frisen/Frisen_gdrive/nbis_frisen_2305"
folder_structure <- get_folder_structure(dir_path, read_files = FALSE)
folder_structure_string <- folder_structure_to_string(folder_structure)
# Initialize the context for ChatGPT instances based on folder structure
context_a <- list(list(role = "system", content = "You are a researcher. Thinking about the data in a project folder and should relevant questions."))
context_a <- append_message(context_a, "user", folder_structure_string)
context_b <- list(list(role = "system", content = "You are an insightful analyst. Answer questions and provide useful information about the folder content."))
# Set temperatures (optional)
temperature_a <- 0.1
temperature_b <- 0.7
# Set the number of interactions and initiate conversation
num_interactions <- 5
current_interaction <- 1
while (current_interaction <= num_interactions) {
# ChatGPT-A asks a question
response_a <- chat_gpt_interaction(context_a, Sys.getenv("OPENAI_API_KEY_NBIS"), model, temperature_a)
message_a <- response_a$response_text
# ChatGPT-B provides an answer
context_b <- append_message(context_b, "user", message_a)
response_b <- chat_gpt_interaction(context_b, Sys.getenv("OPENAI_API_KEY_NBIS"), model, temperature_b)
message_b <- response_b$response_text
# Print the interaction
cat(sprintf("Interaction %d\n", current_interaction))
cat("Question from ChatGPT-A:\n", message_a, "\n")
cat("Answer from ChatGPT-B:\n", message_b, "\n\n")
# Prepare context for the next question
context_a <- append_message(context_a, "assistant", message_b)
# Increment the interaction counter
current_interaction <- current_interaction + 1
}
